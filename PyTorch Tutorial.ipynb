{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/winequality-red-cleaned.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.961877</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>-0.379133</td>\n",
       "      <td>1.288643</td>\n",
       "      <td>-0.579207</td>\n",
       "      <td>-0.960246</td>\n",
       "      <td>5</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.967442</td>\n",
       "      <td>0.043416</td>\n",
       "      <td>0.223875</td>\n",
       "      <td>0.624363</td>\n",
       "      <td>-0.719933</td>\n",
       "      <td>0.128950</td>\n",
       "      <td>-0.584777</td>\n",
       "      <td>5</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.297065</td>\n",
       "      <td>-0.169427</td>\n",
       "      <td>0.096353</td>\n",
       "      <td>0.229047</td>\n",
       "      <td>-0.331177</td>\n",
       "      <td>-0.048089</td>\n",
       "      <td>-0.584777</td>\n",
       "      <td>5</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.384443</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.264960</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>-0.979104</td>\n",
       "      <td>-0.461180</td>\n",
       "      <td>-0.584777</td>\n",
       "      <td>6</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.961877</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>-0.379133</td>\n",
       "      <td>1.288643</td>\n",
       "      <td>-0.579207</td>\n",
       "      <td>-0.960246</td>\n",
       "      <td>5</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   volatile acidity  residual sugar  chlorides  total sulfur dioxide  \\\n",
       "0          0.961877       -0.453218  -0.243707             -0.379133   \n",
       "1          1.967442        0.043416   0.223875              0.624363   \n",
       "2          1.297065       -0.169427   0.096353              0.229047   \n",
       "3         -1.384443       -0.453218  -0.264960              0.411500   \n",
       "4          0.961877       -0.453218  -0.243707             -0.379133   \n",
       "\n",
       "         pH  sulphates   alcohol  quality category  \n",
       "0  1.288643  -0.579207 -0.960246        5      Bad  \n",
       "1 -0.719933   0.128950 -0.584777        5      Bad  \n",
       "2 -0.331177  -0.048089 -0.584777        5      Bad  \n",
       "3 -0.979104  -0.461180 -0.584777        6     Good  \n",
       "4  1.288643  -0.579207 -0.960246        5      Bad  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 0.0, 1.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get labels and store in y variable\n",
    "y = pd.DataFrame([0. if item == 'Good' else 1. for item in df['category']])\n",
    "y = [0. if item == 'Good' else 1. for item in df['category']]\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get features and store in X variable\n",
    "X = df.drop(['quality', 'category'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.961877</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>-0.379133</td>\n",
       "      <td>1.288643</td>\n",
       "      <td>-0.579207</td>\n",
       "      <td>-0.960246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.967442</td>\n",
       "      <td>0.043416</td>\n",
       "      <td>0.223875</td>\n",
       "      <td>0.624363</td>\n",
       "      <td>-0.719933</td>\n",
       "      <td>0.128950</td>\n",
       "      <td>-0.584777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.297065</td>\n",
       "      <td>-0.169427</td>\n",
       "      <td>0.096353</td>\n",
       "      <td>0.229047</td>\n",
       "      <td>-0.331177</td>\n",
       "      <td>-0.048089</td>\n",
       "      <td>-0.584777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.384443</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.264960</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>-0.979104</td>\n",
       "      <td>-0.461180</td>\n",
       "      <td>-0.584777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.961877</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>-0.379133</td>\n",
       "      <td>1.288643</td>\n",
       "      <td>-0.579207</td>\n",
       "      <td>-0.960246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   volatile acidity  residual sugar  chlorides  total sulfur dioxide  \\\n",
       "0          0.961877       -0.453218  -0.243707             -0.379133   \n",
       "1          1.967442        0.043416   0.223875              0.624363   \n",
       "2          1.297065       -0.169427   0.096353              0.229047   \n",
       "3         -1.384443       -0.453218  -0.264960              0.411500   \n",
       "4          0.961877       -0.453218  -0.243707             -0.379133   \n",
       "\n",
       "         pH  sulphates   alcohol  \n",
       "0  1.288643  -0.579207 -0.960246  \n",
       "1 -0.719933   0.128950 -0.584777  \n",
       "2 -0.331177  -0.048089 -0.584777  \n",
       "3 -0.979104  -0.461180 -0.584777  \n",
       "4  1.288643  -0.579207 -0.960246  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kendall/Dropbox/Current_Projects/python/pytorch_tutorial/.direnv/python-3.5.2/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# TODO: pip install sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train = features_df\n",
    "# y_train = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.activation import Softmax\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SingleLayerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SingleLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.softmax(self.fc1(x))\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net0 = SingleLayerNet()\n",
    "optimizer0 = optim.SGD(net0.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DoubleLayerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DoubleLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, 7)\n",
    "        self.fc2 = nn.Linear(7, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.softmax(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net1 = DoubleLayerNet()\n",
    "optimizer1 = optim.SGD(net1.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # use a Classification Cross-Entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_with_net(net, optimizer, criterion, batch_size, \n",
    "                   epochs):\n",
    "    for epoch in range(epochs): # loop over the dataset multiple times\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        for start, end in zip(range(0, len(X_train), batch_size), \n",
    "                              range(batch_size, len(X_train), batch_size)):\n",
    "            # get the inputs\n",
    "            \n",
    "            inputs = torch.from_numpy(X_train[start:end].as_matrix())\n",
    "            inputs = inputs.float()\n",
    "            labels = torch.Tensor(y_train[start:end])\n",
    "            labels = labels.long()\n",
    "    \n",
    "            # wrap them in Variable\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "            \n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "\n",
    "            if epoch % 100 == 99:\n",
    "                print(loss.data[0])\n",
    "                test_outputs = net(Variable(torch.from_numpy(X_test.as_matrix()).float()))\n",
    "                _, predicted = torch.max(test_outputs.data, 1)\n",
    "                accuracy = accuracy_score(y_test, predicted.numpy())\n",
    "                print('%d loss: %.3f, accuracy: %.3f' % (epoch+1, running_loss / 100,\n",
    "                      accuracy))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7567446827888489\n",
      "100 loss: 0.008, accuracy: 0.347\n",
      "0.7302234768867493\n",
      "100 loss: 0.007, accuracy: 0.347\n",
      "0.7375900745391846\n",
      "100 loss: 0.007, accuracy: 0.347\n",
      "0.7461714744567871\n",
      "100 loss: 0.007, accuracy: 0.347\n",
      "0.7207959890365601\n",
      "100 loss: 0.007, accuracy: 0.347\n",
      "0.7488070130348206\n",
      "100 loss: 0.007, accuracy: 0.347\n",
      "0.7610923647880554\n",
      "100 loss: 0.008, accuracy: 0.347\n",
      "0.7600045800209045\n",
      "100 loss: 0.008, accuracy: 0.347\n",
      "0.7349680066108704\n",
      "100 loss: 0.007, accuracy: 0.347\n",
      "0.745857298374176\n",
      "100 loss: 0.007, accuracy: 0.347\n",
      "0.7462821006774902\n",
      "100 loss: 0.007, accuracy: 0.347\n",
      "0.7566758990287781\n",
      "100 loss: 0.008, accuracy: 0.347\n",
      "0.7567446827888489\n",
      "200 loss: 0.008, accuracy: 0.347\n",
      "0.7302234768867493\n",
      "200 loss: 0.007, accuracy: 0.347\n",
      "0.7375900745391846\n",
      "200 loss: 0.007, accuracy: 0.347\n",
      "0.7461714744567871\n",
      "200 loss: 0.007, accuracy: 0.347\n",
      "0.7207959890365601\n",
      "200 loss: 0.007, accuracy: 0.347\n",
      "0.7488070130348206\n",
      "200 loss: 0.007, accuracy: 0.347\n",
      "0.7610923647880554\n",
      "200 loss: 0.008, accuracy: 0.347\n",
      "0.7600045800209045\n",
      "200 loss: 0.008, accuracy: 0.347\n",
      "0.7349680066108704\n",
      "200 loss: 0.007, accuracy: 0.347\n",
      "0.745857298374176\n",
      "200 loss: 0.007, accuracy: 0.347\n",
      "0.7462821006774902\n",
      "200 loss: 0.007, accuracy: 0.347\n",
      "0.7566758990287781\n",
      "200 loss: 0.008, accuracy: 0.347\n",
      "0.7567446827888489\n",
      "300 loss: 0.008, accuracy: 0.347\n",
      "0.7302234768867493\n",
      "300 loss: 0.007, accuracy: 0.347\n",
      "0.7375900745391846\n",
      "300 loss: 0.007, accuracy: 0.347\n",
      "0.7461714744567871\n",
      "300 loss: 0.007, accuracy: 0.347\n",
      "0.7207959890365601\n",
      "300 loss: 0.007, accuracy: 0.347\n",
      "0.7488070130348206\n",
      "300 loss: 0.007, accuracy: 0.347\n",
      "0.7610923647880554\n",
      "300 loss: 0.008, accuracy: 0.347\n",
      "0.7600045800209045\n",
      "300 loss: 0.008, accuracy: 0.347\n",
      "0.7349680066108704\n",
      "300 loss: 0.007, accuracy: 0.347\n",
      "0.745857298374176\n",
      "300 loss: 0.007, accuracy: 0.347\n",
      "0.7462821006774902\n",
      "300 loss: 0.007, accuracy: 0.347\n",
      "0.7566758990287781\n",
      "300 loss: 0.008, accuracy: 0.347\n",
      "0.7567446827888489\n",
      "400 loss: 0.008, accuracy: 0.347\n",
      "0.7302234768867493\n",
      "400 loss: 0.007, accuracy: 0.347\n",
      "0.7375900745391846\n",
      "400 loss: 0.007, accuracy: 0.347\n",
      "0.7461714744567871\n",
      "400 loss: 0.007, accuracy: 0.347\n",
      "0.7207959890365601\n",
      "400 loss: 0.007, accuracy: 0.347\n",
      "0.7488070130348206\n",
      "400 loss: 0.007, accuracy: 0.347\n",
      "0.7610923647880554\n",
      "400 loss: 0.008, accuracy: 0.347\n",
      "0.7600045800209045\n",
      "400 loss: 0.008, accuracy: 0.347\n",
      "0.7349680066108704\n",
      "400 loss: 0.007, accuracy: 0.347\n",
      "0.745857298374176\n",
      "400 loss: 0.007, accuracy: 0.347\n",
      "0.7462821006774902\n",
      "400 loss: 0.007, accuracy: 0.347\n",
      "0.7566758990287781\n",
      "400 loss: 0.008, accuracy: 0.347\n",
      "0.7567446827888489\n",
      "500 loss: 0.008, accuracy: 0.347\n",
      "0.7302234768867493\n",
      "500 loss: 0.007, accuracy: 0.347\n",
      "0.7375900745391846\n",
      "500 loss: 0.007, accuracy: 0.347\n",
      "0.7461714744567871\n",
      "500 loss: 0.007, accuracy: 0.347\n",
      "0.7207959890365601\n",
      "500 loss: 0.007, accuracy: 0.347\n",
      "0.7488070130348206\n",
      "500 loss: 0.007, accuracy: 0.347\n",
      "0.7610923647880554\n",
      "500 loss: 0.008, accuracy: 0.347\n",
      "0.7600045800209045\n",
      "500 loss: 0.008, accuracy: 0.347\n",
      "0.7349680066108704\n",
      "500 loss: 0.007, accuracy: 0.347\n",
      "0.745857298374176\n",
      "500 loss: 0.007, accuracy: 0.347\n",
      "0.7462821006774902\n",
      "500 loss: 0.007, accuracy: 0.347\n",
      "0.7566758990287781\n",
      "500 loss: 0.008, accuracy: 0.347\n",
      "0.7567446827888489\n",
      "600 loss: 0.008, accuracy: 0.347\n",
      "0.7302234768867493\n",
      "600 loss: 0.007, accuracy: 0.347\n",
      "0.7375900745391846\n",
      "600 loss: 0.007, accuracy: 0.347\n",
      "0.7461714744567871\n",
      "600 loss: 0.007, accuracy: 0.347\n",
      "0.7207959890365601\n",
      "600 loss: 0.007, accuracy: 0.347\n",
      "0.7488070130348206\n",
      "600 loss: 0.007, accuracy: 0.347\n",
      "0.7610923647880554\n",
      "600 loss: 0.008, accuracy: 0.347\n",
      "0.7600045800209045\n",
      "600 loss: 0.008, accuracy: 0.347\n",
      "0.7349680066108704\n",
      "600 loss: 0.007, accuracy: 0.347\n",
      "0.745857298374176\n",
      "600 loss: 0.007, accuracy: 0.347\n",
      "0.7462821006774902\n",
      "600 loss: 0.007, accuracy: 0.347\n",
      "0.7566758990287781\n",
      "600 loss: 0.008, accuracy: 0.347\n",
      "0.7567446827888489\n",
      "700 loss: 0.008, accuracy: 0.347\n",
      "0.7302234768867493\n",
      "700 loss: 0.007, accuracy: 0.347\n",
      "0.7375900745391846\n",
      "700 loss: 0.007, accuracy: 0.347\n",
      "0.7461714744567871\n",
      "700 loss: 0.007, accuracy: 0.347\n",
      "0.7207959890365601\n",
      "700 loss: 0.007, accuracy: 0.347\n",
      "0.7488070130348206\n",
      "700 loss: 0.007, accuracy: 0.347\n",
      "0.7610923647880554\n",
      "700 loss: 0.008, accuracy: 0.347\n",
      "0.7600045800209045\n",
      "700 loss: 0.008, accuracy: 0.347\n",
      "0.7349680066108704\n",
      "700 loss: 0.007, accuracy: 0.347\n",
      "0.745857298374176\n",
      "700 loss: 0.007, accuracy: 0.347\n",
      "0.7462821006774902\n",
      "700 loss: 0.007, accuracy: 0.347\n",
      "0.7566758990287781\n",
      "700 loss: 0.008, accuracy: 0.347\n",
      "0.7567446827888489\n",
      "800 loss: 0.008, accuracy: 0.347\n",
      "0.7302234768867493\n",
      "800 loss: 0.007, accuracy: 0.347\n",
      "0.7375900745391846\n",
      "800 loss: 0.007, accuracy: 0.347\n",
      "0.7461714744567871\n",
      "800 loss: 0.007, accuracy: 0.347\n",
      "0.7207959890365601\n",
      "800 loss: 0.007, accuracy: 0.347\n",
      "0.7488070130348206\n",
      "800 loss: 0.007, accuracy: 0.347\n",
      "0.7610923647880554\n",
      "800 loss: 0.008, accuracy: 0.347\n",
      "0.7600045800209045\n",
      "800 loss: 0.008, accuracy: 0.347\n",
      "0.7349680066108704\n",
      "800 loss: 0.007, accuracy: 0.347\n",
      "0.745857298374176\n",
      "800 loss: 0.007, accuracy: 0.347\n",
      "0.7462821006774902\n",
      "800 loss: 0.007, accuracy: 0.347\n",
      "0.7566758990287781\n",
      "800 loss: 0.008, accuracy: 0.347\n",
      "0.7567446827888489\n",
      "900 loss: 0.008, accuracy: 0.347\n",
      "0.7302234768867493\n",
      "900 loss: 0.007, accuracy: 0.347\n",
      "0.7375900745391846\n",
      "900 loss: 0.007, accuracy: 0.347\n",
      "0.7461714744567871\n",
      "900 loss: 0.007, accuracy: 0.347\n",
      "0.7207959890365601\n",
      "900 loss: 0.007, accuracy: 0.347\n",
      "0.7488070130348206\n",
      "900 loss: 0.007, accuracy: 0.347\n",
      "0.7610923647880554\n",
      "900 loss: 0.008, accuracy: 0.347\n",
      "0.7600045800209045\n",
      "900 loss: 0.008, accuracy: 0.347\n",
      "0.7349680066108704\n",
      "900 loss: 0.007, accuracy: 0.347\n",
      "0.745857298374176\n",
      "900 loss: 0.007, accuracy: 0.347\n",
      "0.7462821006774902\n",
      "900 loss: 0.007, accuracy: 0.347\n",
      "0.7566758990287781\n",
      "900 loss: 0.008, accuracy: 0.347\n",
      "0.7567446827888489\n",
      "1000 loss: 0.008, accuracy: 0.347\n",
      "0.7302234768867493\n",
      "1000 loss: 0.007, accuracy: 0.347\n",
      "0.7375900745391846\n",
      "1000 loss: 0.007, accuracy: 0.347\n",
      "0.7461714744567871\n",
      "1000 loss: 0.007, accuracy: 0.347\n",
      "0.7207959890365601\n",
      "1000 loss: 0.007, accuracy: 0.347\n",
      "0.7488070130348206\n",
      "1000 loss: 0.007, accuracy: 0.347\n",
      "0.7610923647880554\n",
      "1000 loss: 0.008, accuracy: 0.347\n",
      "0.7600045800209045\n",
      "1000 loss: 0.008, accuracy: 0.347\n",
      "0.7349680066108704\n",
      "1000 loss: 0.007, accuracy: 0.347\n",
      "0.745857298374176\n",
      "1000 loss: 0.007, accuracy: 0.347\n",
      "0.7462821006774902\n",
      "1000 loss: 0.007, accuracy: 0.347\n",
      "0.7566758990287781\n",
      "1000 loss: 0.008, accuracy: 0.347\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(15)\n",
    "\n",
    "net0 = SingleLayerNet()\n",
    "optimizer0 = optim.SGD(net0.parameters(), lr=learning_rate, momentum=0.9)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "net1 = DoubleLayerNet()\n",
    "optimizer1 = optim.SGD(net1.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "train_with_net(net0, optimizer0, criterion, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6196161508560181\n",
      "100 loss: 0.006, accuracy: 0.738\n",
      "0.5774780511856079\n",
      "100 loss: 0.006, accuracy: 0.741\n",
      "0.5789795517921448\n",
      "100 loss: 0.006, accuracy: 0.741\n",
      "0.532743513584137\n",
      "100 loss: 0.005, accuracy: 0.741\n",
      "0.609984815120697\n",
      "100 loss: 0.006, accuracy: 0.741\n",
      "0.6174442768096924\n",
      "100 loss: 0.006, accuracy: 0.741\n",
      "0.5354151129722595\n",
      "100 loss: 0.005, accuracy: 0.741\n",
      "0.5297208428382874\n",
      "100 loss: 0.005, accuracy: 0.741\n",
      "0.5572843551635742\n",
      "100 loss: 0.006, accuracy: 0.741\n",
      "0.5744524002075195\n",
      "100 loss: 0.006, accuracy: 0.741\n",
      "0.46535712480545044\n",
      "100 loss: 0.005, accuracy: 0.741\n",
      "0.5067234635353088\n",
      "100 loss: 0.005, accuracy: 0.741\n",
      "0.6165396571159363\n",
      "200 loss: 0.006, accuracy: 0.741\n",
      "0.5772078037261963\n",
      "200 loss: 0.006, accuracy: 0.741\n",
      "0.581137478351593\n",
      "200 loss: 0.006, accuracy: 0.741\n",
      "0.528733491897583\n",
      "200 loss: 0.005, accuracy: 0.741\n",
      "0.6111298203468323\n",
      "200 loss: 0.006, accuracy: 0.741\n",
      "0.6165362000465393\n",
      "200 loss: 0.006, accuracy: 0.741\n",
      "0.53464275598526\n",
      "200 loss: 0.005, accuracy: 0.741\n",
      "0.531524658203125\n",
      "200 loss: 0.005, accuracy: 0.741\n",
      "0.5544652938842773\n",
      "200 loss: 0.006, accuracy: 0.744\n",
      "0.5784276723861694\n",
      "200 loss: 0.006, accuracy: 0.741\n",
      "0.45891350507736206\n",
      "200 loss: 0.005, accuracy: 0.741\n",
      "0.5041682720184326\n",
      "200 loss: 0.005, accuracy: 0.741\n",
      "0.6152802109718323\n",
      "300 loss: 0.006, accuracy: 0.741\n",
      "0.5771405696868896\n",
      "300 loss: 0.006, accuracy: 0.741\n",
      "0.5820419788360596\n",
      "300 loss: 0.006, accuracy: 0.741\n",
      "0.5273826122283936\n",
      "300 loss: 0.005, accuracy: 0.741\n",
      "0.6116302609443665\n",
      "300 loss: 0.006, accuracy: 0.741\n",
      "0.6167396306991577\n",
      "300 loss: 0.006, accuracy: 0.741\n",
      "0.5349581837654114\n",
      "300 loss: 0.005, accuracy: 0.741\n",
      "0.5327578186988831\n",
      "300 loss: 0.005, accuracy: 0.741\n",
      "0.5533836483955383\n",
      "300 loss: 0.006, accuracy: 0.741\n",
      "0.5796661972999573\n",
      "300 loss: 0.006, accuracy: 0.741\n",
      "0.45652520656585693\n",
      "300 loss: 0.005, accuracy: 0.741\n",
      "0.5032288432121277\n",
      "300 loss: 0.005, accuracy: 0.741\n",
      "0.6145843863487244\n",
      "400 loss: 0.006, accuracy: 0.738\n",
      "0.5772584080696106\n",
      "400 loss: 0.006, accuracy: 0.738\n",
      "0.5824455618858337\n",
      "400 loss: 0.006, accuracy: 0.738\n",
      "0.526738166809082\n",
      "400 loss: 0.005, accuracy: 0.738\n",
      "0.6120235323905945\n",
      "400 loss: 0.006, accuracy: 0.738\n",
      "0.6170304417610168\n",
      "400 loss: 0.006, accuracy: 0.738\n",
      "0.5352544188499451\n",
      "400 loss: 0.005, accuracy: 0.738\n",
      "0.5334344506263733\n",
      "400 loss: 0.005, accuracy: 0.738\n",
      "0.5527491569519043\n",
      "400 loss: 0.006, accuracy: 0.738\n",
      "0.5801548957824707\n",
      "400 loss: 0.006, accuracy: 0.738\n",
      "0.45524924993515015\n",
      "400 loss: 0.005, accuracy: 0.738\n",
      "0.502956748008728\n",
      "400 loss: 0.005, accuracy: 0.738\n",
      "0.6141461133956909\n",
      "500 loss: 0.006, accuracy: 0.741\n",
      "0.5773693919181824\n",
      "500 loss: 0.006, accuracy: 0.741\n",
      "0.5826729536056519\n",
      "500 loss: 0.006, accuracy: 0.741\n",
      "0.5263916254043579\n",
      "500 loss: 0.005, accuracy: 0.741\n",
      "0.6122680902481079\n",
      "500 loss: 0.006, accuracy: 0.741\n",
      "0.6172454357147217\n",
      "500 loss: 0.006, accuracy: 0.741\n",
      "0.5354561805725098\n",
      "500 loss: 0.005, accuracy: 0.741\n",
      "0.5338480472564697\n",
      "500 loss: 0.005, accuracy: 0.741\n",
      "0.5523446798324585\n",
      "500 loss: 0.006, accuracy: 0.741\n",
      "0.5804170370101929\n",
      "500 loss: 0.006, accuracy: 0.741\n",
      "0.45447471737861633\n",
      "500 loss: 0.005, accuracy: 0.741\n",
      "0.5029171109199524\n",
      "500 loss: 0.005, accuracy: 0.741\n",
      "0.6138548851013184\n",
      "600 loss: 0.006, accuracy: 0.741\n",
      "0.5774471163749695\n",
      "600 loss: 0.006, accuracy: 0.741\n",
      "0.5828189253807068\n",
      "600 loss: 0.006, accuracy: 0.741\n",
      "0.5261906385421753\n",
      "600 loss: 0.005, accuracy: 0.741\n",
      "0.612409770488739\n",
      "600 loss: 0.006, accuracy: 0.741\n",
      "0.617392897605896\n",
      "600 loss: 0.006, accuracy: 0.741\n",
      "0.5355893969535828\n",
      "600 loss: 0.005, accuracy: 0.741\n",
      "0.5341203212738037\n",
      "600 loss: 0.005, accuracy: 0.741\n",
      "0.5520780086517334\n",
      "600 loss: 0.006, accuracy: 0.741\n",
      "0.5805844664573669\n",
      "600 loss: 0.006, accuracy: 0.741\n",
      "0.4539759159088135\n",
      "600 loss: 0.005, accuracy: 0.741\n",
      "0.5029481649398804\n",
      "600 loss: 0.005, accuracy: 0.741\n",
      "0.6136562824249268\n",
      "700 loss: 0.006, accuracy: 0.741\n",
      "0.577499508857727\n",
      "700 loss: 0.006, accuracy: 0.741\n",
      "0.5829182267189026\n",
      "700 loss: 0.006, accuracy: 0.741\n",
      "0.5260669589042664\n",
      "700 loss: 0.005, accuracy: 0.741\n",
      "0.6124936938285828\n",
      "700 loss: 0.006, accuracy: 0.741\n",
      "0.6174933314323425\n",
      "700 loss: 0.006, accuracy: 0.741\n",
      "0.5356788039207458\n",
      "700 loss: 0.005, accuracy: 0.741\n",
      "0.5343062877655029\n",
      "700 loss: 0.005, accuracy: 0.741\n",
      "0.5518977046012878\n",
      "700 loss: 0.006, accuracy: 0.741\n",
      "0.5806983113288879\n",
      "700 loss: 0.006, accuracy: 0.741\n",
      "0.4536425769329071\n",
      "700 loss: 0.005, accuracy: 0.741\n",
      "0.5029943585395813\n",
      "700 loss: 0.005, accuracy: 0.741\n",
      "0.6135186553001404\n",
      "800 loss: 0.006, accuracy: 0.741\n",
      "0.5775353312492371\n",
      "800 loss: 0.006, accuracy: 0.741\n",
      "0.5829870700836182\n",
      "800 loss: 0.006, accuracy: 0.741\n",
      "0.5259872078895569\n",
      "800 loss: 0.005, accuracy: 0.741\n",
      "0.612545907497406\n",
      "800 loss: 0.006, accuracy: 0.741\n",
      "0.6175625920295715\n",
      "800 loss: 0.006, accuracy: 0.741\n",
      "0.5357397198677063\n",
      "800 loss: 0.005, accuracy: 0.741\n",
      "0.534435510635376\n",
      "800 loss: 0.005, accuracy: 0.741\n",
      "0.5517732501029968\n",
      "800 loss: 0.006, accuracy: 0.741\n",
      "0.5807774066925049\n",
      "800 loss: 0.006, accuracy: 0.741\n",
      "0.4534147381782532\n",
      "800 loss: 0.005, accuracy: 0.741\n",
      "0.5030375123023987\n",
      "800 loss: 0.005, accuracy: 0.741\n",
      "0.6134219169616699\n",
      "900 loss: 0.006, accuracy: 0.741\n",
      "0.5775600671768188\n",
      "900 loss: 0.006, accuracy: 0.741\n",
      "0.5830354690551758\n",
      "900 loss: 0.006, accuracy: 0.741\n",
      "0.5259341597557068\n",
      "900 loss: 0.005, accuracy: 0.741\n",
      "0.6125797629356384\n",
      "900 loss: 0.006, accuracy: 0.741\n",
      "0.6176109313964844\n",
      "900 loss: 0.006, accuracy: 0.741\n",
      "0.5357820987701416\n",
      "900 loss: 0.005, accuracy: 0.741\n",
      "0.5345265865325928\n",
      "900 loss: 0.005, accuracy: 0.741\n",
      "0.551686704158783\n",
      "900 loss: 0.006, accuracy: 0.741\n",
      "0.5808332562446594\n",
      "900 loss: 0.006, accuracy: 0.741\n",
      "0.4532564878463745\n",
      "900 loss: 0.005, accuracy: 0.741\n",
      "0.5030727982521057\n",
      "900 loss: 0.005, accuracy: 0.741\n",
      "0.6133536100387573\n",
      "1000 loss: 0.006, accuracy: 0.741\n",
      "0.5775773525238037\n",
      "1000 loss: 0.006, accuracy: 0.741\n",
      "0.5830695033073425\n",
      "1000 loss: 0.006, accuracy: 0.741\n",
      "0.5258980393409729\n",
      "1000 loss: 0.005, accuracy: 0.741\n",
      "0.6126025915145874\n",
      "1000 loss: 0.006, accuracy: 0.741\n",
      "0.6176449060440063\n",
      "1000 loss: 0.006, accuracy: 0.741\n",
      "0.5358117818832397\n",
      "1000 loss: 0.005, accuracy: 0.741\n",
      "0.5345909595489502\n",
      "1000 loss: 0.005, accuracy: 0.741\n",
      "0.5516256093978882\n",
      "1000 loss: 0.006, accuracy: 0.741\n",
      "0.5808725357055664\n",
      "1000 loss: 0.006, accuracy: 0.741\n",
      "0.453145295381546\n",
      "1000 loss: 0.005, accuracy: 0.741\n",
      "0.5031000971794128\n",
      "1000 loss: 0.005, accuracy: 0.741\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_with_net(net1, optimizer1, criterion, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "outputs = net0(Variable(torch.from_numpy(X_test.as_matrix()).float()))\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "confusion = confusion_matrix(y_test, predicted.numpy())\n",
    "accuracy = accuracy_score(y_test, predicted.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6304273009300232\n",
      "10 loss: 0.063, accuracy: 0.738\n",
      "0.5905195474624634\n",
      "10 loss: 0.059, accuracy: 0.734\n",
      "0.6098040342330933\n",
      "10 loss: 0.061, accuracy: 0.734\n",
      "0.5754178166389465\n",
      "10 loss: 0.058, accuracy: 0.731\n",
      "0.6121941208839417\n",
      "10 loss: 0.061, accuracy: 0.731\n",
      "0.5987313985824585\n",
      "10 loss: 0.060, accuracy: 0.731\n",
      "0.5567293763160706\n",
      "10 loss: 0.056, accuracy: 0.731\n",
      "0.5624191164970398\n",
      "10 loss: 0.056, accuracy: 0.734\n",
      "0.5905687212944031\n",
      "10 loss: 0.059, accuracy: 0.734\n",
      "0.5943408608436584\n",
      "10 loss: 0.059, accuracy: 0.734\n",
      "0.5206661820411682\n",
      "10 loss: 0.052, accuracy: 0.734\n",
      "0.5427858233451843\n",
      "10 loss: 0.054, accuracy: 0.734\n",
      "0.6213106513023376\n",
      "20 loss: 0.062, accuracy: 0.744\n",
      "0.5795345902442932\n",
      "20 loss: 0.058, accuracy: 0.744\n",
      "0.5891777873039246\n",
      "20 loss: 0.059, accuracy: 0.744\n",
      "0.5489501357078552\n",
      "20 loss: 0.055, accuracy: 0.744\n",
      "0.6055160760879517\n",
      "20 loss: 0.061, accuracy: 0.741\n",
      "0.6046742796897888\n",
      "20 loss: 0.060, accuracy: 0.744\n",
      "0.5444614291191101\n",
      "20 loss: 0.054, accuracy: 0.744\n",
      "0.5403470396995544\n",
      "20 loss: 0.054, accuracy: 0.741\n",
      "0.5746804475784302\n",
      "20 loss: 0.057, accuracy: 0.741\n",
      "0.5808840990066528\n",
      "20 loss: 0.058, accuracy: 0.741\n",
      "0.49441927671432495\n",
      "20 loss: 0.049, accuracy: 0.741\n",
      "0.5208914279937744\n",
      "20 loss: 0.052, accuracy: 0.741\n",
      "0.6201808452606201\n",
      "30 loss: 0.062, accuracy: 0.738\n",
      "0.577212929725647\n",
      "30 loss: 0.058, accuracy: 0.734\n",
      "0.5832326412200928\n",
      "30 loss: 0.058, accuracy: 0.738\n",
      "0.5404557585716248\n",
      "30 loss: 0.054, accuracy: 0.741\n",
      "0.6058990955352783\n",
      "30 loss: 0.061, accuracy: 0.738\n",
      "0.6095016598701477\n",
      "30 loss: 0.061, accuracy: 0.738\n",
      "0.5406832695007324\n",
      "30 loss: 0.054, accuracy: 0.734\n",
      "0.5339159369468689\n",
      "30 loss: 0.053, accuracy: 0.738\n",
      "0.5684360265731812\n",
      "30 loss: 0.057, accuracy: 0.738\n",
      "0.5775454640388489\n",
      "30 loss: 0.058, accuracy: 0.738\n",
      "0.48349541425704956\n",
      "30 loss: 0.048, accuracy: 0.738\n",
      "0.5134229063987732\n",
      "30 loss: 0.051, accuracy: 0.734\n",
      "0.6200894713401794\n",
      "40 loss: 0.062, accuracy: 0.741\n",
      "0.5764968991279602\n",
      "40 loss: 0.058, accuracy: 0.741\n",
      "0.581063449382782\n",
      "40 loss: 0.058, accuracy: 0.741\n",
      "0.536673367023468\n",
      "40 loss: 0.054, accuracy: 0.741\n",
      "0.6070131659507751\n",
      "40 loss: 0.061, accuracy: 0.741\n",
      "0.6120206117630005\n",
      "40 loss: 0.061, accuracy: 0.741\n",
      "0.5387791395187378\n",
      "40 loss: 0.054, accuracy: 0.741\n",
      "0.5315850973129272\n",
      "40 loss: 0.053, accuracy: 0.741\n",
      "0.5649552345275879\n",
      "40 loss: 0.056, accuracy: 0.738\n",
      "0.5764528512954712\n",
      "40 loss: 0.058, accuracy: 0.741\n",
      "0.4773552417755127\n",
      "40 loss: 0.048, accuracy: 0.741\n",
      "0.5099697709083557\n",
      "40 loss: 0.051, accuracy: 0.741\n",
      "0.620007336139679\n",
      "50 loss: 0.062, accuracy: 0.741\n",
      "0.576253354549408\n",
      "50 loss: 0.058, accuracy: 0.741\n",
      "0.5802085399627686\n",
      "50 loss: 0.058, accuracy: 0.741\n",
      "0.534541666507721\n",
      "50 loss: 0.053, accuracy: 0.741\n",
      "0.6080567836761475\n",
      "50 loss: 0.061, accuracy: 0.741\n",
      "0.6133608818054199\n",
      "50 loss: 0.061, accuracy: 0.741\n",
      "0.5375863313674927\n",
      "50 loss: 0.054, accuracy: 0.741\n",
      "0.5306544899940491\n",
      "50 loss: 0.053, accuracy: 0.741\n",
      "0.5627081990242004\n",
      "50 loss: 0.056, accuracy: 0.741\n",
      "0.5761116743087769\n",
      "50 loss: 0.058, accuracy: 0.741\n",
      "0.47335487604141235\n",
      "50 loss: 0.047, accuracy: 0.741\n",
      "0.5080261826515198\n",
      "50 loss: 0.051, accuracy: 0.741\n",
      "0.6198239326477051\n",
      "60 loss: 0.062, accuracy: 0.741\n",
      "0.576200544834137\n",
      "60 loss: 0.058, accuracy: 0.741\n",
      "0.5798940658569336\n",
      "60 loss: 0.058, accuracy: 0.741\n",
      "0.5331405997276306\n",
      "60 loss: 0.053, accuracy: 0.741\n",
      "0.6089256405830383\n",
      "60 loss: 0.061, accuracy: 0.741\n",
      "0.6141311526298523\n",
      "60 loss: 0.061, accuracy: 0.741\n",
      "0.5367732048034668\n",
      "60 loss: 0.054, accuracy: 0.741\n",
      "0.5302937626838684\n",
      "60 loss: 0.053, accuracy: 0.741\n",
      "0.561133861541748\n",
      "60 loss: 0.056, accuracy: 0.741\n",
      "0.5760882496833801\n",
      "60 loss: 0.058, accuracy: 0.741\n",
      "0.47051623463630676\n",
      "60 loss: 0.047, accuracy: 0.741\n",
      "0.5067822933197021\n",
      "60 loss: 0.051, accuracy: 0.741\n",
      "0.6195681095123291\n",
      "70 loss: 0.062, accuracy: 0.741\n",
      "0.5762372612953186\n",
      "70 loss: 0.058, accuracy: 0.741\n",
      "0.5798246264457703\n",
      "70 loss: 0.058, accuracy: 0.741\n",
      "0.5321266651153564\n",
      "70 loss: 0.053, accuracy: 0.741\n",
      "0.6096280813217163\n",
      "70 loss: 0.061, accuracy: 0.741\n",
      "0.6146166324615479\n",
      "70 loss: 0.061, accuracy: 0.741\n",
      "0.536199152469635\n",
      "70 loss: 0.054, accuracy: 0.741\n",
      "0.5301933884620667\n",
      "70 loss: 0.053, accuracy: 0.741\n",
      "0.559963583946228\n",
      "70 loss: 0.056, accuracy: 0.741\n",
      "0.5762127041816711\n",
      "70 loss: 0.058, accuracy: 0.741\n",
      "0.46838149428367615\n",
      "70 loss: 0.047, accuracy: 0.741\n",
      "0.5059159994125366\n",
      "70 loss: 0.051, accuracy: 0.738\n",
      "0.6192757487297058\n",
      "80 loss: 0.062, accuracy: 0.738\n",
      "0.5763166546821594\n",
      "80 loss: 0.058, accuracy: 0.738\n",
      "0.5798726677894592\n",
      "80 loss: 0.058, accuracy: 0.738\n",
      "0.5313465595245361\n",
      "80 loss: 0.053, accuracy: 0.738\n",
      "0.6101914048194885\n",
      "80 loss: 0.061, accuracy: 0.738\n",
      "0.6149541735649109\n",
      "80 loss: 0.061, accuracy: 0.738\n",
      "0.5357882380485535\n",
      "80 loss: 0.054, accuracy: 0.741\n",
      "0.5302218794822693\n",
      "80 loss: 0.053, accuracy: 0.738\n",
      "0.5590525269508362\n",
      "80 loss: 0.056, accuracy: 0.738\n",
      "0.5764073133468628\n",
      "80 loss: 0.058, accuracy: 0.738\n",
      "0.4667052924633026\n",
      "80 loss: 0.047, accuracy: 0.738\n",
      "0.5052762627601624\n",
      "80 loss: 0.051, accuracy: 0.738\n",
      "0.6189713478088379\n",
      "90 loss: 0.062, accuracy: 0.738\n",
      "0.5764135122299194\n",
      "90 loss: 0.058, accuracy: 0.738\n",
      "0.5799782872200012\n",
      "90 loss: 0.058, accuracy: 0.734\n",
      "0.530722975730896\n",
      "90 loss: 0.053, accuracy: 0.738\n",
      "0.610641360282898\n",
      "90 loss: 0.061, accuracy: 0.738\n",
      "0.6152093410491943\n",
      "90 loss: 0.062, accuracy: 0.738\n",
      "0.535493791103363\n",
      "90 loss: 0.054, accuracy: 0.738\n",
      "0.5303184390068054\n",
      "90 loss: 0.053, accuracy: 0.738\n",
      "0.558317244052887\n",
      "90 loss: 0.056, accuracy: 0.738\n",
      "0.5766326785087585\n",
      "90 loss: 0.058, accuracy: 0.741\n",
      "0.4653450846672058\n",
      "90 loss: 0.047, accuracy: 0.738\n",
      "0.5047855973243713\n",
      "90 loss: 0.050, accuracy: 0.738\n",
      "0.6186688542366028\n",
      "100 loss: 0.062, accuracy: 0.738\n",
      "0.5765141844749451\n",
      "100 loss: 0.058, accuracy: 0.738\n",
      "0.5801117420196533\n",
      "100 loss: 0.058, accuracy: 0.738\n",
      "0.5302108526229858\n",
      "100 loss: 0.053, accuracy: 0.738\n",
      "0.611000120639801\n",
      "100 loss: 0.061, accuracy: 0.738\n",
      "0.6154158115386963\n",
      "100 loss: 0.062, accuracy: 0.738\n",
      "0.5352834463119507\n",
      "100 loss: 0.054, accuracy: 0.738\n",
      "0.5304518342018127\n",
      "100 loss: 0.053, accuracy: 0.738\n",
      "0.557706892490387\n",
      "100 loss: 0.056, accuracy: 0.738\n",
      "0.5768679976463318\n",
      "100 loss: 0.058, accuracy: 0.738\n",
      "0.464213103055954\n",
      "100 loss: 0.046, accuracy: 0.738\n",
      "0.5043994784355164\n",
      "100 loss: 0.050, accuracy: 0.738\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "class FourLayerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FourLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, 7)\n",
    "        self.fc2 = nn.Linear(7, 2)\n",
    "        self.fc3 = nn.Linear(7, 2)\n",
    "        self.fc4 = nn.Linear(7, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(self.fc4(x))\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net2 = DoubleLayerNet()\n",
    "optimizer2 = optim.SGD(net2.parameters(), lr=learning_rate, momentum=0.0)\n",
    "train_with_net(net2, optimizer2, criterion, batch_size, epochs, epochs_to_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**MNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist1d = fetch_mldata('MNIST original', data_home='./Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist1d.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist2d = np.reshape(mnist1d.data, (70000, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 28, 28)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist2d.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MNISTImageNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTImageNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool  = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "mnistnet = MNISTImageNet()\n",
    "mnistoptimizer = optim.SGD(mnistnet.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-0663342d6ce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtrain_with_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnistnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnistoptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-98-0663342d6ce8>\u001b[0m in \u001b[0;36mtrain_with_net\u001b[0;34m(net, optimizer, criterion, batch_size, epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kendall/Dropbox/Current_Projects/python/pytorch_tutorial/.direnv/python-3.5.2/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-1b34a7deda6a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kendall/Dropbox/Current_Projects/python/pytorch_tutorial/.direnv/python-3.5.2/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kendall/Dropbox/Current_Projects/python/pytorch_tutorial/.direnv/python-3.5.2/lib/python3.5/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 235\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kendall/Dropbox/Current_Projects/python/pytorch_tutorial/.direnv/python-3.5.2/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     35\u001b[0m     f = ConvNd(_pair(stride), _pair(padding), _pair(dilation), False,\n\u001b[1;32m     36\u001b[0m                _pair(0), groups)\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kendall/Dropbox/Current_Projects/python/pytorch_tutorial/.direnv/python-3.5.2/lib/python3.5/site-packages/torch/nn/_functions/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_view4d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kendall/Dropbox/Current_Projects/python/pytorch_tutorial/.direnv/python-3.5.2/lib/python3.5/site-packages/torch/nn/_functions/conv.py\u001b[0m in \u001b[0;36m_view4d\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_with_net(net, optimizer, criterion, batch_size, \n",
    "                   epochs):\n",
    "    for epoch in range(epochs): # loop over the dataset multiple times\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        for start, end in zip(range(0, len(mnist2d.data), batch_size), \n",
    "                              range(batch_size, len(mnist2d.data), batch_size)):\n",
    "            # get the inputs\n",
    "            \n",
    "            inputs = torch.from_numpy(mnist2d[start:end])\n",
    "            inputs = inputs.float()\n",
    "            labels = torch.from_numpy(mnist1d.target[start:end])\n",
    "            labels = labels.long()\n",
    "    \n",
    "            # wrap them in Variable\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "            \n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "\n",
    "            if epoch % 100 == 99:\n",
    "                print(loss.data[0])\n",
    "                test_outputs = net(Variable(torch.from_numpy(X_test.as_matrix()).float()))\n",
    "                _, predicted = torch.max(test_outputs.data, 1)\n",
    "                accuracy = accuracy_score(y_test, predicted.numpy())\n",
    "                print('%d loss: %.3f, accuracy: %.3f' % (epoch+1, running_loss / 100,\n",
    "                      accuracy))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "train_with_net(mnistnet, mnistoptimizer, criterion, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
